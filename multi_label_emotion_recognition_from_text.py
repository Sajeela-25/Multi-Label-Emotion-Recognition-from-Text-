# -*- coding: utf-8 -*-
"""Multi-Label Emotion Recognition from Text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i73bLVD85i9ElYdb7B2DXnu2HqUNQNq3
"""

# Step 0: (Optional) install dependencies
!pip install -q imbalanced-learn

!pip install datasets

from google.colab import files
uploaded = files.upload()

!pip install transformers datasets scikit-learn torch

!pip install -U transformers

!pip uninstall -y torch torchvision torchaudio
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 1. Imports
import re
import os
import torch
import pandas as pd
from torch.nn import BCEWithLogitsLoss
from sklearn.metrics import f1_score, hamming_loss
from scipy.special import expit
from datasets import Dataset
import numpy as np
from transformers import (
    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,
    Trainer, TrainingArguments
)

# Disable Weights & Biases logging
os.environ["WANDB_DISABLED"] = "true"

# 2. Load & Clean Dataset
# Load TSV GoEmotions file
df = pd.read_csv("go_emotions_dataset.csv") # remove sep='\t'
# Drop unclear examples if the column exists
if 'example_very_unclear' in df.columns:
    df = df[~df["example_very_unclear"]].drop(columns=["example_very_unclear"])
# Basic text cleaning
def clean_text(txt):
    txt = str(txt).lower()
    txt = re.sub(r"http\S+|www\S+", "", txt)
    txt = re.sub(r"[^a-z0-9\s']", " ", txt)
    return re.sub(r"\s+", " ", txt).strip()

df["text_clean"] = df["text"].apply(clean_text)

# 3. Labels & Class Weights
emotion_cols = [
    "admiration","amusement","anger","annoyance","approval","caring",
    "confusion","curiosity","desire","disappointment","disapproval",
    "disgust","embarrassment","excitement","fear","gratitude","grief",
    "joy","love","nervousness","optimism","pride","realization","relief",
    "remorse","sadness","surprise","neutral"
]
# Compute pos_weight for imbalanced labels
y = df[emotion_cols].astype(int)
N = len(df)
pos_counts = y.sum(axis=0).values
neg_counts = N - pos_counts
pos_weights = torch.tensor(neg_counts / pos_counts, dtype=torch.float)

# 4. Prepare Hugging Face Dataset
hf = Dataset.from_pandas(df[["text_clean"] + emotion_cols])

def pack_labels(example):
    example["labels"] = [float(example[c]) for c in emotion_cols]
    return example

hf = hf.map(pack_labels, remove_columns=emotion_cols)
# Split into train/test
data = hf.train_test_split(test_size=0.1, seed=42)
# Optional: limit size for faster experiments
data["train"] = data["train"].select(range(10000))
data["test"] = data["test"].select(range(2000))

# 5. Tokenization
model_name = "bert-base-uncased"
MAX_LENGTH = 128
BATCH_SIZE = 64
EPOCHS = 50

tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(example):
    return tokenizer(
        example["text_clean"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH,
    )

data = data.map(tokenize, batched=True)
# Set torch format
data.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# 6. Model Setup & Freeze Encoder
config = AutoConfig.from_pretrained(
    model_name,
    num_labels=len(emotion_cols),
    problem_type="multi_label_classification"
)
model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)
# Freeze base BERT weights
for name, param in model.named_parameters():
    if name.startswith("bert.encoder") or name.startswith("bert.embeddings"):
        param.requires_grad = False

# 7 Metrics
def compute_metrics(pred):
    logits = pred.predictions
    labels = pred.label_ids
    probs = expit(logits)
    preds = (probs > 0.5).astype(int)
    return {
        "micro_f1": f1_score(labels, preds, average="micro"),
        "macro_f1": f1_score(labels, preds, average="macro"),
    }

# 8. Custom Trainer with Weighted Loss
class BertTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop("labels").float().to(model.device)
        outputs = model(**inputs)
        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weights.to(model.device))
        loss = loss_fn(outputs.logits, labels)
        return (loss, outputs) if return_outputs else loss

# 9 TrainingArguments (fp16, 2 epochs, no midâ€‘train eval)
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=3e-5,
    per_device_train_batch_size=BATCH_SIZE,      # Now properly defined
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    fp16=True,
    logging_steps=50,
    save_strategy="no",
    optim="adamw_torch",
    report_to="none"  # Disables external logging
)
# 10 Train & evaluate
trainer = BertTrainer(
    model=model,
    args=training_args,
    train_dataset=data["train"],
    eval_dataset=data["test"],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
trainer.save_model("./results")  # Saves model and config
tokenizer.save_pretrained("./results")  # Also save tokenizer

# Put model in eval mode
model.eval()

all_preds = []
all_labels = []

# Iterate through eval data
for batch in data['test']:
    input_ids = batch['input_ids'].unsqueeze(0).to(model.device)
    attention_mask = batch['attention_mask'].unsqueeze(0).to(model.device)
    labels = batch['labels'].unsqueeze(0).to(model.device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probs = expit(logits.cpu().numpy())
        preds = (probs > 0.5).astype(int)

    all_preds.append(preds[0])
    all_labels.append(labels.cpu().numpy()[0])

# Convert to numpy arrays
all_preds = np.array(all_preds)
all_labels = np.array(all_labels)

# Compute metrics
micro_f1 = f1_score(all_labels, all_preds, average="micro")
macro_f1 = f1_score(all_labels, all_preds, average="macro")
hamming = hamming_loss(all_labels, all_preds)

print(f"\nEvaluation Model")
print(f"Micro F1: {micro_f1:.4f}")
print(f"Macro F1: {macro_f1:.4f}")
print(f"Hamming Loss: {hamming:.4f}")

# 11 Inference on Real-World Texts
def predict_emotions(texts, threshold=0.5):
    inputs = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
    model.eval()
    with torch.no_grad():
        logits = model(**inputs.to(model.device)).logits
        probs = torch.sigmoid(logits).cpu().numpy()
    return [[emotion for emotion, p in zip(emotion_cols, ps) if p > threshold] or ["neutral"] for ps in probs]

if __name__ == "__main__":
    print("\nEnter text to analyze (type 'quit' to exit):")
    while True:
        user_text = input("Text: ")
        if user_text.lower() in ["quit", "exit"]:
            break
        preds = predict_emotions([user_text])[0]
        print(f"Predicted emotions: {preds}\n")